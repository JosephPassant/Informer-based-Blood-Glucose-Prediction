{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **REPLACE-BG DATA PROCESSING**   \n",
    "- **Without feature enhancement**  \n",
    "  \n",
    "- **2:1:2 hypo:eu:hyper sampling ratio**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CONTENTS**\n",
    "\n",
    "[1. Requirements & Environment](#1-requirements--environment)  \n",
    "[2. Read in Replace-BG Dataset](#2-read-in-replace-bg-dataset)  \n",
    "[3. Initial Processing & Train/Validation/Test Split](#3-initial-processing--trainvalidationtest-split)  \n",
    "[4. Training Data Processing](#4-training-data-processing)  \n",
    "[5. Validation Data Processing](#5-validation-data-processing)  \n",
    "[6. Test Data Processing](#6-test-data-processing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Requirements & Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data_processing_modules import *\n",
    "from data_processing_parameters import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#CONTENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Read in Replace-BG Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using a relative path from current working directory\n",
    "replace_bg_path = os.path.join('..', 'source_data', 'SourceData', 'ReplaceBG', 'Data_tables', 'hdevicecgm.txt')\n",
    "\n",
    "# Read the data\n",
    "replace_cgm_data = pd.read_csv(replace_bg_path, delimiter='|')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#CONTENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.    ReplaceBG Initial Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes calibration data / direct blood glucose measurements from dataset leaving only CGM data\n",
    "replace_cgm_data = replace_cgm_data[replace_cgm_data['RecordType'] == 'CGM']\n",
    "\n",
    "# Drop columns that will not be required\n",
    "replace_cgm_data = replace_cgm_data.drop(columns=['RecID', 'ParentHDeviceUploadsID', 'SiteID', 'DexInternalDtTmDaysFromEnroll', 'DexInternalTm', 'RecordType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by ptid, then devicedttmdaysfromenroll then devicetm to separate the data into individual patient time series sequences\n",
    "replace_cgm_data = replace_cgm_data.sort_values(by=['PtID', 'DeviceDtTmDaysFromEnroll', 'DeviceTm']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the DeviceDtTmDaysFromEnroll to the base date\n",
    "# base date imported from data_processing_parameters.py\n",
    "replace_cgm_data['DateTime'] = base_date + pd.to_timedelta(replace_cgm_data['DeviceDtTmDaysFromEnroll'], unit='D')\n",
    "\n",
    "# add device time to the date time to get a full datetime stamp\n",
    "replace_cgm_data['DateTime'] = replace_cgm_data['DateTime'] + pd.to_timedelta(replace_cgm_data['DeviceTm'])\n",
    "\n",
    "# Drop DeviceDtTmDaysFromEnroll column as no longer needed\n",
    "replace_cgm_data = replace_cgm_data.drop(columns=['DeviceDtTmDaysFromEnroll'])\n",
    "\n",
    "# Ensure Data is still sorted by Patient ID and DateTime\n",
    "replace_cgm_data = replace_cgm_data.sort_values(by=['PtID', 'DateTime'], ascending= [True, True])\n",
    "\n",
    "# Drop DeviceTm column as no longer needed\n",
    "replace_cgm_data = replace_cgm_data.drop(columns=['DeviceTm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate replace_cgm into individual patient time series dfs\n",
    "replace_cgm_data_dict = separate_ptid_data(replace_cgm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardises the earliest date for each patient\n",
    "replace_cgm_data_dict = align_start_date(replace_cgm_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, validation and test datasets for each patient maintaining the time series order\n",
    "replace_cgm_training_data = {}\n",
    "replace_cgm_validation_data = {}\n",
    "replace_cgm_test_data = {}\n",
    "\n",
    "for ptid, df in replace_cgm_data_dict.items():\n",
    "    train, test = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "    train, val = train_test_split(train, test_size=0.2, shuffle=False)\n",
    "    replace_cgm_training_data[ptid] = train\n",
    "    replace_cgm_validation_data[ptid] = val\n",
    "    replace_cgm_test_data[ptid] = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#CONTENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.    ReplaceBG Training Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolates data points for single misssing values\n",
    "\n",
    "for ptid, df in replace_cgm_training_data.items():\n",
    "    df = df.copy()\n",
    "    df['real_value_flag'] = 1\n",
    "    df['TimeDiff'] = df['DateTime'].diff().dt.total_seconds()\n",
    "\n",
    "    # Identify rows where TimeDiff is around 600 seconds (10 min)\n",
    "    mask = (df['TimeDiff'] > 595) & (df['TimeDiff'] < 605)\n",
    "    insert_rows = df[mask].copy()\n",
    "\n",
    "    if not insert_rows.empty:\n",
    "        # Modify new rows: set `real_value_flag = 0`, shift `DateTime`, and set `GlucoseValue = NaN`\n",
    "        insert_rows['real_value_flag'] = 0\n",
    "        insert_rows['DateTime'] -= pd.to_timedelta(5, unit='m')\n",
    "        insert_rows['GlucoseValue'] = np.nan\n",
    "\n",
    "    # Append new rows to the dataframe and sort into correct chronological order\n",
    "    df = pd.concat([df, insert_rows]).sort_values(by='DateTime').reset_index(drop=True)\n",
    "\n",
    "    # Linearly interpolate the glucose value for the added rows\n",
    "    df['GlucoseValue'] = df['GlucoseValue'].interpolate(method='linear')\n",
    "\n",
    "    # create hour and minute columns\n",
    "    df['Hour'] = df['DateTime'].dt.hour\n",
    "    df['Minute'] = df['DateTime'].dt.minute\n",
    "\n",
    "    # creates a rolling sum to determins complete data sequences suitable for use\n",
    "    # data sequences that hold missing data over  gaps greater that 10 minutes 5 seconds will be excluded\n",
    "    df['TimeDiff'] = df['DateTime'].diff().dt.total_seconds()\n",
    "    df['TimeDiffFlag'] = df['TimeDiff'].apply(lambda x: 0 if x < 295 or x > 305 else 1)\n",
    "    df['RollingTimeDiffFlag'] = df['TimeDiffFlag'].rolling(window=96).sum()\n",
    "\n",
    "    # drop columns\n",
    "    df = df.drop(columns=['DateTime', 'TimeDiff', 'TimeDiffFlag', 'real_value_flag'])\n",
    "\n",
    "    # replace the initial df with the new df\n",
    "    replace_cgm_training_data[ptid] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create slices of total input length\n",
    "\n",
    "replace_training_slices = []\n",
    "\n",
    "for ptid, df in replace_cgm_training_data.items():\n",
    "    rolling_flag_array = df[\"RollingTimeDiffFlag\"].to_numpy()  # Convert to NumPy array for fast indexing\n",
    "    num_rows = len(df)\n",
    "    starting_index = 0\n",
    "\n",
    "    while starting_index + slice_size <= num_rows:\n",
    "        if rolling_flag_array[starting_index + slice_size - 1] == 96:  # Use precomputed array\n",
    "            replace_training_slices.append(df.iloc[starting_index:starting_index + slice_size])\n",
    "            starting_index += 2  # Move by overlap\n",
    "        else:\n",
    "            starting_index += 1  # Ensure progress to avoid infinite loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143709, 1242100, 975918)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split slices into hypo, eu and hyper slices\n",
    "\n",
    "hypo_training_slices = []\n",
    "eu_training_slices = []\n",
    "hyper_training_slices = []\n",
    "\n",
    "for slice in replace_training_slices:\n",
    "\n",
    "    target_glucose_values = slice.iloc[-target_size:]['GlucoseValue'].values\n",
    "\n",
    "    hypo_value_count = np.sum(target_glucose_values < 70)\n",
    "    eu_value_count = np.sum((target_glucose_values >= 70) & (target_glucose_values <= 180))\n",
    "    hyper_value_count = np.sum(target_glucose_values > 180)\n",
    "\n",
    "    # minimum points required for a slice to be classed as hypo or hyper\n",
    "    min_points = 6\n",
    "\n",
    "    if hypo_value_count >= min_points:\n",
    "        hypo_training_slices.append(slice)\n",
    "    elif hyper_value_count >= min_points:\n",
    "        hyper_training_slices.append(slice)\n",
    "    else:\n",
    "        eu_training_slices.append(slice)\n",
    "\n",
    "\n",
    "len(hypo_training_slices), len(eu_training_slices), len(hyper_training_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21349, 22009, 22506, 44330)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# profile hypo slices by where the hypo occurs in the slice to ensure even distribution\n",
    "\n",
    "_030_slices = []\n",
    "_3060_slices = []\n",
    "_6090_slices = []\n",
    "_90120_slices = []\n",
    "\n",
    "for slice in hypo_training_slices:\n",
    "\n",
    "    _030_values = slice.iloc[-24:-18]['GlucoseValue'].values\n",
    "    _3060_values = slice.iloc[-18:-12]['GlucoseValue'].values\n",
    "    _6090_values = slice.iloc[-12:-6]['GlucoseValue'].values\n",
    "    _90120_values = slice.iloc[-6:]['GlucoseValue'].values\n",
    "\n",
    "    _030_count = np.sum(_030_values < 70)\n",
    "    _3060_count = np.sum(_3060_values < 70)\n",
    "    _6090_count = np.sum(_6090_values < 70)\n",
    "    _90120_count = np.sum(_90120_values < 70)\n",
    "\n",
    "    min_points = 6\n",
    "\n",
    "    if _90120_count >= min_points:\n",
    "        _90120_slices.append(slice)\n",
    "    elif _6090_count >= min_points:\n",
    "        _6090_slices.append(slice)\n",
    "    elif _3060_count >= min_points:\n",
    "        _3060_slices.append(slice)\n",
    "    elif _030_count >= min_points:\n",
    "        _030_slices.append(slice)\n",
    "\n",
    "len(_030_slices), len(_3060_slices),len(_6090_slices), len(_90120_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110194"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiled_hypo_slices = _030_slices + _3060_slices + _6090_slices + _90120_slices\n",
    "\n",
    "len(profiled_hypo_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_training_dict = {idx: slice for idx, slice in enumerate(eu_training_slices)}\n",
    "hyper_training_dict = {idx: slice for idx, slice in enumerate(hyper_training_slices)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = int(len(profiled_hypo_slices)/2)\n",
    "eu_training_dict = undersample_dict(eu_training_dict, target_size)\n",
    "eu_training_list = list(eu_training_dict.values())\n",
    "\n",
    "target_size = len(profiled_hypo_slices)\n",
    "\n",
    "hyper_training_dict = undersample_dict(hyper_training_dict, target_size)\n",
    "hyper_training_list = list(hyper_training_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55097, 110194)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eu_training_list), len(hyper_training_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_slice_list = profiled_hypo_slices + eu_training_list + hyper_training_list\n",
    "\n",
    "random.shuffle(training_slice_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 152.91051040286524, Std: 70.27050122812615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#merge all replace traing slices into one dataframe to calculate mean and std\n",
    "training_df = pd.concat(training_slice_list, ignore_index=True)\n",
    "\n",
    "# Get the mean and standard deviation for the training dataset\n",
    "training_mean = training_df['GlucoseValue'].mean()\n",
    "training_std = training_df['GlucoseValue'].std()\n",
    "print(f\"Mean: {training_mean}, Std: {training_std}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise the training slices\n",
    "for i in range(len(training_slice_list)):\n",
    "    training_slice_list[i] = training_slice_list[i].copy() \n",
    "    training_slice_list[i].drop(columns=['RollingTimeDiffFlag', 'PtID'], inplace=True, errors='ignore')\n",
    "    training_slice_list[i].loc[:, 'GlucoseValue'] = (training_slice_list[i]['GlucoseValue'] - training_mean) / training_std  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GlucoseValue</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>-0.980646</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>-0.937954</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>-0.952185</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>-0.937954</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>-0.881031</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GlucoseValue  Hour  Minute\n",
       "747     -0.980646     0      56\n",
       "748     -0.937954     1       1\n",
       "749     -0.952185     1       6\n",
       "750     -0.937954     1      11\n",
       "751     -0.881031     1      16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_slice_list[0].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dir = '../processed_data/replace_bg/baseline_no_feature_enhancement_211/training/encoder_slices'\n",
    "decoder_dir = '../processed_data/replace_bg/baseline_no_feature_enhancement_211/training/decoder_slices'\n",
    "target_dir = '../processed_data/replace_bg/baseline_no_feature_enhancement_211/training/target_slices'\n",
    "\n",
    "os.makedirs(encoder_dir, exist_ok=True)\n",
    "os.makedirs(decoder_dir, exist_ok=True)\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "for count, slice in enumerate(training_slice_list):\n",
    "    # Define Encoder, Decoder, and Target sequences (Avoid Copying)\n",
    "    encoder_input = slice.iloc[:encoder_input_size]\n",
    "    target = slice.iloc[encoder_input_size: ]['GlucoseValue']\n",
    "\n",
    "    # Set target sequence to 0 in decoder_input after starting_token (Modify in place)\n",
    "    decoder_input = slice.iloc[-decoder_input_size:].copy().reset_index(drop=True)\n",
    "    # SET[STARTING_TOKEN:] TO 0 FOR DECODER INPUT\n",
    "    decoder_input.loc[decoder_input.index[start_token_size:], 'GlucoseValue'] = 0\n",
    "\n",
    "    # Define file paths\n",
    "    encoder_path = os.path.join(encoder_dir, f\"{count}.pt\")\n",
    "    decoder_path = os.path.join(decoder_dir, f\"{count}.pt\")\n",
    "    target_path = os.path.join(target_dir, f\"{count}.pt\")\n",
    "\n",
    "    # Save tensors without unnecessary copies\n",
    "    torch.save(torch.tensor(encoder_input.values, dtype=torch.float32), encoder_path)\n",
    "    torch.save(torch.tensor(decoder_input.values, dtype=torch.float32), decoder_path)\n",
    "    torch.save(torch.tensor(target.values, dtype=torch.float32), target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Encoder Shape: (72, 3)\n",
      "   GlucoseValue  Hour  Minute\n",
      "0     -0.752955  21.0    39.0\n",
      "1     -0.752955  21.0    44.0\n",
      "2     -0.781416  21.0    49.0\n",
      "3     -0.795647  21.0    54.0\n",
      "4     -0.809878  21.0    59.0\n",
      "\n",
      " Decoder Shape: (36, 3)\n",
      "    GlucoseValue  Hour  Minute\n",
      "6      -1.208338   3.0     9.0\n",
      "7      -1.137184   3.0    14.0\n",
      "8      -1.051800   3.0    19.0\n",
      "9      -1.009108   3.0    24.0\n",
      "10     -0.966416   3.0    29.0\n",
      "11     -0.952185   3.0    34.0\n",
      "12      0.000000   3.0    39.0\n",
      "13      0.000000   3.0    44.0\n",
      "14      0.000000   3.0    49.0\n",
      "15      0.000000   3.0    54.0\n",
      "16      0.000000   3.0    59.0\n",
      "17      0.000000   4.0     4.0\n",
      "18      0.000000   4.0     9.0\n",
      "19      0.000000   4.0    14.0\n",
      "20      0.000000   4.0    19.0\n",
      "21      0.000000   4.0    24.0\n",
      "22      0.000000   4.0    29.0\n",
      "23      0.000000   4.0    34.0\n",
      "24      0.000000   4.0    39.0\n",
      "25      0.000000   4.0    44.0\n",
      "26      0.000000   4.0    49.0\n",
      "27      0.000000   4.0    54.0\n",
      "28      0.000000   4.0    59.0\n",
      "29      0.000000   5.0     4.0\n",
      "30      0.000000   5.0     9.0\n",
      "31      0.000000   5.0    14.0\n",
      "32      0.000000   5.0    19.0\n",
      "33      0.000000   5.0    24.0\n",
      "34      0.000000   5.0    29.0\n",
      "35      0.000000   5.0    34.0\n",
      "\n",
      " Target Shape: (24, 1)\n",
      "    GlucoseValue\n",
      "19     -1.379107\n",
      "20     -1.364876\n",
      "21     -1.364876\n",
      "22     -1.393337\n",
      "23     -1.393337\n"
     ]
    }
   ],
   "source": [
    "encoder_file = get_first_file(encoder_dir)\n",
    "decoder_file = get_first_file(decoder_dir)\n",
    "target_file = get_first_file(target_dir)\n",
    "\n",
    "encoder_tensor = torch.load(encoder_file)\n",
    "decoder_tensor = torch.load(decoder_file)\n",
    "target_tensor = torch.load(target_file)\n",
    "\n",
    "\n",
    "encoder_df = pd.DataFrame(encoder_tensor.numpy(), columns=[\"GlucoseValue\", 'Hour', 'Minute'])\n",
    "decoder_df = pd.DataFrame(decoder_tensor.numpy(), columns=[\"GlucoseValue\", 'Hour', 'Minute'])\n",
    "target_df = pd.DataFrame(target_tensor.numpy(), columns=[\"GlucoseValue\"])\n",
    "\n",
    "print(f\"\\n Encoder Shape: {encoder_df.shape}\")\n",
    "print(encoder_df.head())\n",
    "print(f\"\\n Decoder Shape: {decoder_df.shape}\")\n",
    "print(decoder_df.tail(30))\n",
    "print(f\"\\n Target Shape: {target_df.shape}\")\n",
    "print(target_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#CONTENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.    ReplaceBG Validation Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ptid, df in replace_cgm_validation_data.items():\n",
    "    df = df.copy()\n",
    "    df['real_value_flag'] = 1\n",
    "    df['TimeDiff'] = df['DateTime'].diff().dt.total_seconds()\n",
    "\n",
    "    # Identify rows where TimeDiff is around 600 seconds (10 min)\n",
    "    mask = (df['TimeDiff'] > 595) & (df['TimeDiff'] < 605)\n",
    "    insert_rows = df[mask].copy()\n",
    "\n",
    "    if not insert_rows.empty:\n",
    "        # Modify new rows: set `real_value_flag = 0`, shift `DateTime`, and set `GlucoseValue = NaN`\n",
    "        insert_rows['real_value_flag'] = 0\n",
    "        insert_rows['DateTime'] -= pd.to_timedelta(5, unit='m')\n",
    "        insert_rows['GlucoseValue'] = np.nan\n",
    "\n",
    "    # Append new rows to the dataframe and sort\n",
    "    df = pd.concat([df, insert_rows]).sort_values(by='DateTime').reset_index(drop=True)\n",
    "\n",
    "    # Linearly interpolate the glucose value\n",
    "    df['GlucoseValue'] = df['GlucoseValue'].interpolate(method='linear')\n",
    "\n",
    "    # create hour and minute columns\n",
    "    df['Hour'] = df['DateTime'].dt.hour\n",
    "    df['Minute'] = df['DateTime'].dt.minute\n",
    "\n",
    "    # creates a rolling sum to determins complete data sequences suitable for use\n",
    "    # data sequences that hold missing data over  gaps greater that 10 minutes 5 seconds will be excluded\n",
    "    df['TimeDiff'] = df['DateTime'].diff().dt.total_seconds()\n",
    "    df['TimeDiffFlag'] = df['TimeDiff'].apply(lambda x: 0 if x < 295 or x > 305 else 1)\n",
    "    df['RollingTimeDiffFlag'] = df['TimeDiffFlag'].rolling(window=slice_size).sum()\n",
    "\n",
    "    # drop columns\n",
    "    df = df.drop(columns=['DateTime', 'TimeDiff', 'TimeDiffFlag', 'real_value_flag'])\n",
    "\n",
    "    replace_cgm_validation_data[ptid] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create slices of total input length\n",
    "\n",
    "replace_validation_slices = []\n",
    "\n",
    "for ptid, df in replace_cgm_validation_data.items():\n",
    "    if 'RollingTimeDiffFlag' not in df.columns:\n",
    "        continue  # Skip this dataframe if the column does not exist\n",
    "    rolling_flag_array = df[\"RollingTimeDiffFlag\"].to_numpy()  # Convert to NumPy array for fast indexing\n",
    "    num_rows = len(df)\n",
    "    starting_index = 0\n",
    "\n",
    "    while starting_index + slice_size <= num_rows:\n",
    "        if rolling_flag_array[starting_index + slice_size - 1] == slice_size:  # Use precomputed array\n",
    "            replace_validation_slices.append(df.iloc[starting_index:starting_index + slice_size])\n",
    "            starting_index += overlap  # Move by overlap\n",
    "        else:\n",
    "            starting_index += 1  # Ensure progress to avoid infinite loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255136"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replace_validation_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dict = {idx: slice for idx, slice in enumerate(replace_validation_slices)}\n",
    "\n",
    "target_size = int(len(validation_dict)/2)\n",
    "\n",
    "undersampled_validation_dict = undersample_dict(validation_dict, target_size)\n",
    "\n",
    "validation_list = list(undersampled_validation_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127568"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(validation_list)):\n",
    "    validation_list[i] = validation_list[i].copy() \n",
    "    validation_list[i].drop(columns=['RollingTimeDiffFlag', 'PtID'], inplace=True, errors='ignore')\n",
    "    validation_list[i].loc[:, 'GlucoseValue'] = (validation_list[i]['GlucoseValue'] - training_mean) / training_std  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GlucoseValue</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>-0.269110</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>-0.240649</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>-0.212187</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>-0.197957</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>-0.197957</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      GlucoseValue  Hour  Minute\n",
       "1554     -0.269110     2      18\n",
       "1555     -0.240649     2      23\n",
       "1556     -0.212187     2      28\n",
       "1557     -0.197957     2      33\n",
       "1558     -0.197957     2      38"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dir = '../processed_data/replace_bg/baseline_no_feature_enhancement_211_undersample/validation/encoder_slices'\n",
    "decoder_dir = '../processed_data/replace_bg/baseline_no_feature_enhancement_211_undersample/validation/decoder_slices'\n",
    "target_dir = '../processed_data/replace_bg/baseline_no_feature_enhancement_211_undersample/validation/target_slices'\n",
    "\n",
    "\n",
    "os.makedirs(encoder_dir, exist_ok=True)\n",
    "os.makedirs(decoder_dir, exist_ok=True)\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "for count, slice in enumerate(validation_list):\n",
    "    # Define Encoder, Decoder, and Target sequences (Avoid Copying)\n",
    "    encoder_input = slice.iloc[:encoder_input_size]\n",
    "    target = slice.iloc[encoder_input_size: ]['GlucoseValue']\n",
    "\n",
    "    # Set target sequence to 0 in decoder_input after starting_token (Modify in place)\n",
    "    decoder_input = slice.iloc[-decoder_input_size:].copy().reset_index(drop=True)\n",
    "    # SET[STARTING_TOKEN:] TO 0 FOR DECODER INPUT\n",
    "    decoder_input.loc[decoder_input.index[start_token_size:], 'GlucoseValue'] = 0\n",
    "\n",
    "    # Define file paths\n",
    "    encoder_path = os.path.join(encoder_dir, f\"{count}.pt\")\n",
    "    decoder_path = os.path.join(decoder_dir, f\"{count}.pt\")\n",
    "    target_path = os.path.join(target_dir, f\"{count}.pt\")\n",
    "\n",
    "    # Save tensors without unnecessary copies\n",
    "    torch.save(torch.tensor(encoder_input.values, dtype=torch.float32), encoder_path)\n",
    "    torch.save(torch.tensor(decoder_input.values, dtype=torch.float32), decoder_path)\n",
    "    torch.save(torch.tensor(target.values, dtype=torch.float32), target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Encoder Shape: (72, 3)\n",
      "    GlucoseValue  Hour  Minute\n",
      "67      3.274340   5.0    38.0\n",
      "68      3.174725   5.0    43.0\n",
      "69      3.046648   5.0    48.0\n",
      "70      2.961264   5.0    53.0\n",
      "71      2.804726   5.0    58.0\n",
      "\n",
      " Decoder Shape: (36, 3)\n",
      "    GlucoseValue  Hour  Minute\n",
      "6       3.317032   5.0    33.0\n",
      "7       3.274340   5.0    38.0\n",
      "8       3.174725   5.0    43.0\n",
      "9       3.046648   5.0    48.0\n",
      "10      2.961264   5.0    53.0\n",
      "11      2.804726   5.0    58.0\n",
      "12      0.000000   6.0     3.0\n",
      "13      0.000000   6.0     8.0\n",
      "14      0.000000   6.0    13.0\n",
      "15      0.000000   6.0    18.0\n",
      "16      0.000000   6.0    23.0\n",
      "17      0.000000   6.0    28.0\n",
      "18      0.000000   6.0    33.0\n",
      "19      0.000000   6.0    38.0\n",
      "20      0.000000   6.0    43.0\n",
      "21      0.000000   6.0    48.0\n",
      "22      0.000000   6.0    53.0\n",
      "23      0.000000   6.0    58.0\n",
      "24      0.000000   7.0     3.0\n",
      "25      0.000000   7.0     8.0\n",
      "26      0.000000   7.0    13.0\n",
      "27      0.000000   7.0    18.0\n",
      "28      0.000000   7.0    23.0\n",
      "29      0.000000   7.0    28.0\n",
      "30      0.000000   7.0    33.0\n",
      "31      0.000000   7.0    38.0\n",
      "32      0.000000   7.0    43.0\n",
      "33      0.000000   7.0    48.0\n",
      "34      0.000000   7.0    53.0\n",
      "35      0.000000   7.0    58.0\n",
      "\n",
      " Target Shape: (24, 1)\n",
      "    GlucoseValue\n",
      "19     -1.379107\n",
      "20     -1.364876\n",
      "21     -1.364876\n",
      "22     -1.393337\n",
      "23     -1.393337\n"
     ]
    }
   ],
   "source": [
    "encoder_file = get_first_file(encoder_dir)\n",
    "decoder_file = get_first_file(decoder_dir)\n",
    "target_file = get_first_file(target_dir)\n",
    "\n",
    "encoder_tensor = torch.load(encoder_file)\n",
    "decoder_tensor = torch.load(decoder_file)\n",
    "target_tensor = torch.load(target_file)\n",
    "\n",
    "encoder_df = pd.DataFrame(encoder_tensor.numpy(), columns=[\"GlucoseValue\", 'Hour', 'Minute'])\n",
    "decoder_df = pd.DataFrame(decoder_tensor.numpy(), columns=[\"GlucoseValue\", 'Hour', 'Minute'])\n",
    "target = pd.DataFrame(target_tensor.numpy(), columns=[\"GlucoseValue\"])\n",
    "\n",
    "print(f\"\\n Encoder Shape: {encoder_df.shape}\")\n",
    "print(encoder_df.tail())\n",
    "print(f\"\\n Decoder Shape: {decoder_df.shape}\")\n",
    "print(decoder_df.tail(30))\n",
    "print(f\"\\n Target Shape: {target_df.shape}\")\n",
    "print(target_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#CONTENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.    ReplaceBG Test Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ptid, df in replace_cgm_test_data.items():\n",
    "    df = df.copy()\n",
    "    df['real_value_flag'] = 1\n",
    "    df['TimeDiff'] = df['DateTime'].diff().dt.total_seconds()\n",
    "\n",
    "    # Identify rows where TimeDiff is around 600 seconds (10 min)\n",
    "    mask = (df['TimeDiff'] > 595) & (df['TimeDiff'] < 605)\n",
    "    insert_rows = df[mask].copy()\n",
    "\n",
    "    if not insert_rows.empty:\n",
    "        # Modify new rows: set `real_value_flag = 0`, shift `DateTime`, and set `GlucoseValue = NaN`\n",
    "        insert_rows['real_value_flag'] = 0\n",
    "        insert_rows['DateTime'] -= pd.to_timedelta(5, unit='m')\n",
    "        insert_rows['GlucoseValue'] = np.nan\n",
    "\n",
    "        # Append new rows to the dataframe and sort\n",
    "    df = pd.concat([df, insert_rows]).sort_values(by='DateTime').reset_index(drop=True)\n",
    "\n",
    "    # Linearly interpolate the glucose value\n",
    "    df['GlucoseValue'] = df['GlucoseValue'].interpolate(method='linear')\n",
    "    df['Hour'] = df['DateTime'].dt.hour\n",
    "    df['Minute'] = df['DateTime'].dt.minute\n",
    "    df['TimeDiff'] = df['DateTime'].diff().dt.total_seconds()\n",
    "    df['TimeDiffFlag'] = df['TimeDiff'].apply(lambda x: 0 if x < 295 or x > 305 else 1)\n",
    "    df['RollingTimeDiffFlag'] = df['TimeDiffFlag'].rolling(window=96).sum()\n",
    "\n",
    "    # drop columns\n",
    "    df = df.drop(columns=['DateTime', 'TimeDiff', 'TimeDiffFlag', 'real_value_flag'])\n",
    "\n",
    "    replace_cgm_test_data[ptid] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_test_slices = []\n",
    "\n",
    "for ptid, df in replace_cgm_test_data.items():\n",
    "    if 'RollingTimeDiffFlag' not in df.columns:\n",
    "        continue  # Skip this dataframe if the column does not exist\n",
    "    rolling_flag_array = df[\"RollingTimeDiffFlag\"].to_numpy()  # Convert to NumPy array for fast indexing\n",
    "    num_rows = len(df)\n",
    "    starting_index = 0\n",
    "\n",
    "    while starting_index + slice_size <= num_rows:\n",
    "        if rolling_flag_array[starting_index + slice_size - 1] == slice_size:  # Use precomputed array\n",
    "            replace_test_slices.append(df.iloc[starting_index:starting_index + slice_size])\n",
    "            starting_index += overlap  # Move by overlap\n",
    "        else:\n",
    "            starting_index += 1  # Ensure progress to avoid infinite loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283792"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replace_test_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {idx: slice for idx, slice in enumerate(replace_test_slices)}\n",
    "\n",
    "target_size = int(len(test_dict)/2)\n",
    "\n",
    "undersampled_test_dict = undersample_dict(test_dict, target_size)\n",
    "\n",
    "test_list = list(undersampled_test_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_list)):\n",
    "    test_list[i] = test_list[i].copy() \n",
    "    test_list[i].drop(columns=['RollingTimeDiffFlag', 'PtID'], inplace=True, errors='ignore')\n",
    "    test_list[i].loc[:, 'GlucoseValue'] = (test_list[i]['GlucoseValue'] - training_mean) / training_std  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GlucoseValue</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7097</th>\n",
       "      <td>0.058196</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7098</th>\n",
       "      <td>-0.041419</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7099</th>\n",
       "      <td>-0.141034</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7100</th>\n",
       "      <td>-0.169495</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7101</th>\n",
       "      <td>-0.169495</td>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      GlucoseValue  Hour  Minute\n",
       "7097      0.058196    21      12\n",
       "7098     -0.041419    21      17\n",
       "7099     -0.141034    21      22\n",
       "7100     -0.169495    21      27\n",
       "7101     -0.169495    21      32"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "encoder_dir = '../processed_data/replace_bg/baseline_no_feature_enhancement_211_undersample/testing/encoder_slices'\n",
    "decoder_dir = '../processed_data/replace_bg/baseline_no_feature_enhancement_211_undersample/testing/decoder_slices'\n",
    "target_dir = '../processed_data/replace_bg/baseline_no_feature_enhancement_211_undersample/testing/target_slices'\n",
    "\n",
    "os.makedirs(encoder_dir, exist_ok=True)\n",
    "os.makedirs(decoder_dir, exist_ok=True)\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for count, slice in enumerate(test_list):\n",
    "    # Define Encoder, Decoder, and Target sequences (Avoid Copying)\n",
    "    encoder_input = slice.iloc[:encoder_input_size]\n",
    "    target = slice.iloc[encoder_input_size: ]['GlucoseValue']\n",
    "\n",
    "    # Set target sequence to 0 in decoder_input after starting_token (Modify in place)\n",
    "    decoder_input = slice.iloc[-decoder_input_size:].copy().reset_index(drop=True)\n",
    "    # SET[STARTING_TOKEN:] TO 0 FOR DECODER INPUT\n",
    "    decoder_input.loc[decoder_input.index[start_token_size:], 'GlucoseValue'] = 0\n",
    "\n",
    "    # Define file paths\n",
    "    encoder_path = os.path.join(encoder_dir, f\"{count}.pt\")\n",
    "    decoder_path = os.path.join(decoder_dir, f\"{count}.pt\")\n",
    "    target_path = os.path.join(target_dir, f\"{count}.pt\")\n",
    "\n",
    "    # Save tensors without unnecessary copies\n",
    "    torch.save(torch.tensor(encoder_input.values, dtype=torch.float32), encoder_path)\n",
    "    torch.save(torch.tensor(decoder_input.values, dtype=torch.float32), decoder_path)\n",
    "    torch.save(torch.tensor(target.values, dtype=torch.float32), target_path)\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Encoder Shape: (72, 3)\n",
      "    GlucoseValue  Hour  Minute\n",
      "67     -0.439879   2.0    45.0\n",
      "68     -0.468340   2.0    50.0\n",
      "69     -0.482571   2.0    55.0\n",
      "70     -0.482571   3.0     0.0\n",
      "71     -0.468340   3.0     5.0\n",
      "\n",
      " Decoder Shape: (36, 3)\n",
      "    GlucoseValue  Hour  Minute\n",
      "6      -0.454110   2.0    40.0\n",
      "7      -0.439879   2.0    45.0\n",
      "8      -0.468340   2.0    50.0\n",
      "9      -0.482571   2.0    55.0\n",
      "10     -0.482571   3.0     0.0\n",
      "11     -0.468340   3.0     5.0\n",
      "12      0.000000   3.0    10.0\n",
      "13      0.000000   3.0    15.0\n",
      "14      0.000000   3.0    20.0\n",
      "15      0.000000   3.0    25.0\n",
      "16      0.000000   3.0    30.0\n",
      "17      0.000000   3.0    35.0\n",
      "18      0.000000   3.0    40.0\n",
      "19      0.000000   3.0    45.0\n",
      "20      0.000000   3.0    50.0\n",
      "21      0.000000   3.0    55.0\n",
      "22      0.000000   4.0     0.0\n",
      "23      0.000000   4.0     5.0\n",
      "24      0.000000   4.0    10.0\n",
      "25      0.000000   4.0    15.0\n",
      "26      0.000000   4.0    20.0\n",
      "27      0.000000   4.0    25.0\n",
      "28      0.000000   4.0    30.0\n",
      "29      0.000000   4.0    35.0\n",
      "30      0.000000   4.0    40.0\n",
      "31      0.000000   4.0    45.0\n",
      "32      0.000000   4.0    50.0\n",
      "33      0.000000   4.0    55.0\n",
      "34      0.000000   5.0     0.0\n",
      "35      0.000000   5.0     5.0\n",
      "\n",
      " Target Shape: (24, 1)\n",
      "    GlucoseValue\n",
      "19     -1.379107\n",
      "20     -1.364876\n",
      "21     -1.364876\n",
      "22     -1.393337\n",
      "23     -1.393337\n"
     ]
    }
   ],
   "source": [
    "encoder_file = get_first_file(encoder_dir)\n",
    "decoder_file = get_first_file(decoder_dir)\n",
    "target_file = get_first_file(target_dir)\n",
    "\n",
    "encoder_tensor = torch.load(encoder_file)\n",
    "decoder_tensor = torch.load(decoder_file)\n",
    "target_tensor = torch.load(target_file)\n",
    "\n",
    "encoder_df = pd.DataFrame(encoder_tensor.numpy(), columns=[\"GlucoseValue\", 'Hour', 'Minute'])\n",
    "decoder_df = pd.DataFrame(decoder_tensor.numpy(), columns=[\"GlucoseValue\", 'Hour', 'Minute'])\n",
    "target = pd.DataFrame(target_tensor.numpy(), columns=[\"GlucoseValue\"])\n",
    "\n",
    "print(f\"\\n Encoder Shape: {encoder_df.shape}\")\n",
    "print(encoder_df.tail())\n",
    "print(f\"\\n Decoder Shape: {decoder_df.shape}\")\n",
    "print(decoder_df.tail(30))\n",
    "print(f\"\\n Target Shape: {target_df.shape}\")\n",
    "print(target_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#CONTENTS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
